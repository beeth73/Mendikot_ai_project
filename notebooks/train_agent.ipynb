{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5c3d57-e830-493f-989c-f52004dbd9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting device to CPU for local execution.\n",
      "Changed working directory to: D:\\mendikot_ai_project\n",
      "Directory 'D:/mendikot_ai_project/models' already exists.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "#               LOCAL SETUP CELL\n",
    "#  This cell configures the environment for local execution.\n",
    "# ==========================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Define the device\n",
    "#    Locally, we will use the CPU.\n",
    "print(\"Setting device to CPU for local execution.\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 2. Set the working directory (optional but good practice)\n",
    "#    This helps ensure paths like 'models/' work as expected.\n",
    "#    If your notebook is in the 'notebooks' subfolder, you might need to go up one level.\n",
    "try:\n",
    "    # This tries to change directory to the project root if you run from 'notebooks' folder\n",
    "    os.chdir('..')\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    # If it fails, you are likely already in the root directory.\n",
    "    print(f\"Already in project root directory: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "# 3. Create the 'models' directory if it doesn't exist\n",
    "models_dir = 'D:/mendikot_ai_project/models'\n",
    "if not os.path.exists(models_dir):\n",
    "    print(f\"Directory '{models_dir}' not found. Creating it...\")\n",
    "    os.makedirs(models_dir)\n",
    "else:\n",
    "    print(f\"Directory '{models_dir}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02205605-df3c-4d5e-9873-f052ae6f9128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hyperparameters Set for Training ---\n",
      "State Size: 272, Action Size: 12\n",
      "Total Episodes: 50000\n",
      "----------------------------------------\n",
      "--- Starting new training run: 2025-10-25_09-10-03 ---\n",
      "Models will be saved in: models\\2025-10-25_09-10-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 73/50000 [01:51<20:08:46,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "#              FINAL ALL-IN-ONE PRO AGENT TRAINING SCRIPT\n",
    "# ====================================================================\n",
    "\n",
    "# ------------------- Part 1: IMPORTS -------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import product\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ------------------- Part 2: HYPERPARAMETERS -------------------\n",
    "NUM_EPISODES = 50000\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "SAVE_EVERY = 1000\n",
    "# CRITICAL: Updated state size for the Pro Agent\n",
    "STATE_SIZE = 272\n",
    "ACTION_SIZE = 12\n",
    "\n",
    "# ------------------- Part 3: HELPER CLASSES & FUNCTIONS -------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, policy, reward): self.buffer.append((state, policy, reward))\n",
    "    def sample(self, batch_size): return random.sample(self.buffer, batch_size)\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "RANKS_48 = [str(r) for r in range(3, 11)] + ['J', 'Q', 'K', 'A']\n",
    "SUITS = ['H', 'D', 'C', 'S']\n",
    "CARD_TO_ID = {f\"{rank}{suit}\": i for i, (rank, suit) in enumerate(product(RANKS_48, SUITS))}\n",
    "ID_TO_CARD = {i: card for card, i in CARD_TO_ID.items()}\n",
    "def get_deck_48(): return list(range(48))\n",
    "def get_rank_suit_from_id(card_id):\n",
    "    if not (0 <= card_id < 48): raise ValueError(\"Invalid card ID\")\n",
    "    rank_idx = card_id // 4\n",
    "    suit_idx = card_id % 4\n",
    "    return RANKS_48[rank_idx], SUITS[suit_idx]\n",
    "\n",
    "\n",
    "# ------------------- Part 4: MAIN GAME AND AI CLASSES -------------------\n",
    "class GameState:\n",
    "    def __init__(self, num_players=4, dealer_index=0):\n",
    "        self.num_players = num_players\n",
    "        if num_players == 4: self.cards_per_player = 12\n",
    "        else: raise ValueError(\"Only 4 players are supported by this version.\")\n",
    "        \n",
    "        self.dealer_index = dealer_index\n",
    "        self.current_player_index = (dealer_index + 1) % num_players\n",
    "        self.deck = get_deck_48()\n",
    "        self.hands = {i: [] for i in range(num_players)}\n",
    "        self.trick_cards, self.trick_suits_led, self.trump_suit, self.trump_declarer_team = [], set(), None, None\n",
    "        self.mendis_captured, self.tricks_captured = {0: 0, 1: 0}, {0: 0, 1: 0}\n",
    "        self.mendi_suit_ids = set()\n",
    "        self.current_trick_lead_player = (dealer_index + 1) % num_players\n",
    "        self.current_trick_lead_suit = None\n",
    "        self.card_owner_history = np.zeros(48, dtype=int)\n",
    "        self.void_suits = {p: set() for p in range(num_players)}\n",
    "        self._initialize_mendi_ids()\n",
    "        self.deal()\n",
    "\n",
    "    def _initialize_mendi_ids(self):\n",
    "        for suit_str in SUITS: self.mendi_suit_ids.add(CARD_TO_ID[f\"10{suit_str}\"])\n",
    "    \n",
    "    def deal(self):\n",
    "        random.shuffle(self.deck)\n",
    "        for i, card_id in enumerate(self.deck): self.hands[i % self.num_players].append(card_id)\n",
    "        for hand in self.hands.values(): hand.sort()\n",
    "\n",
    "    def get_team(self, player_idx): return player_idx % 2\n",
    "\n",
    "    def get_legal_moves(self):\n",
    "        player_hand = self.hands[self.current_player_index]\n",
    "        if not player_hand: return []\n",
    "        if not self.trick_cards:\n",
    "            if sum(self.tricks_captured.values()) < 4 and len(self.trick_suits_led) < 4:\n",
    "                possible_leads = [c for c in player_hand if get_rank_suit_from_id(c)[1] not in self.trick_suits_led]\n",
    "                return possible_leads if possible_leads else player_hand\n",
    "            return player_hand\n",
    "        lead_suit = self.current_trick_lead_suit\n",
    "        cards_of_lead_suit = [c for c in player_hand if get_rank_suit_from_id(c)[1] == lead_suit]\n",
    "        return cards_of_lead_suit if cards_of_lead_suit else player_hand\n",
    "\n",
    "    def play_card(self, player_idx, card_id):\n",
    "        if player_idx != self.current_player_index: raise ValueError(f\"Not Player {player_idx}'s turn.\")\n",
    "        if card_id not in self.hands[player_idx]: raise ValueError(f\"Player {player_idx} does not have card {card_id}.\")\n",
    "\n",
    "        if self.trump_suit is None:\n",
    "            if self.current_trick_lead_suit is not None:\n",
    "                _, played_suit = get_rank_suit_from_id(card_id)\n",
    "                can_follow = any(get_rank_suit_from_id(c)[1] == self.current_trick_lead_suit for c in self.hands[player_idx])\n",
    "                if not can_follow and played_suit != self.current_trick_lead_suit:\n",
    "                    self.set_trump(card_id, player_idx)\n",
    "        \n",
    "        self.card_owner_history[card_id] = player_idx + 1\n",
    "        if self.current_trick_lead_suit is not None:\n",
    "            _, played_suit = get_rank_suit_from_id(card_id)\n",
    "            if played_suit != self.current_trick_lead_suit:\n",
    "                self.void_suits[player_idx].add(self.current_trick_lead_suit)\n",
    "        \n",
    "        self.hands[player_idx].remove(card_id)\n",
    "        self.trick_cards.append((player_idx, card_id))\n",
    "        \n",
    "        if len(self.trick_cards) == 1:\n",
    "            self.current_trick_lead_player = player_idx\n",
    "            _, lead_suit = get_rank_suit_from_id(card_id)\n",
    "            self.current_trick_lead_suit = lead_suit\n",
    "            self.trick_suits_led.add(lead_suit)\n",
    "        \n",
    "        if len(self.trick_cards) == self.num_players: self._resolve_trick()\n",
    "        else: self.current_player_index = (self.current_player_index + 1) % self.num_players\n",
    "\n",
    "    def _resolve_trick(self):\n",
    "        lead_suit = self.current_trick_lead_suit\n",
    "        winning_player, lead_card = self.trick_cards[0]\n",
    "        highest_trump, highest_lead_suit = -1, lead_card\n",
    "        for p_id, c_id in self.trick_cards:\n",
    "            _, suit = get_rank_suit_from_id(c_id)\n",
    "            if suit == self.trump_suit and c_id > highest_trump:\n",
    "                highest_trump, winning_player = c_id, p_id\n",
    "        if highest_trump == -1:\n",
    "            for p_id, c_id in self.trick_cards:\n",
    "                _, suit = get_rank_suit_from_id(c_id)\n",
    "                if suit == lead_suit and c_id > highest_lead_suit:\n",
    "                    highest_lead_suit, winning_player = c_id, p_id\n",
    "        self.win_trick(winning_player)\n",
    "        self.trick_cards.clear()\n",
    "        self.current_trick_lead_suit = None\n",
    "\n",
    "    def win_trick(self, winner_idx):\n",
    "        winner_team = self.get_team(winner_idx)\n",
    "        self.tricks_captured[winner_team] += 1\n",
    "        for _, c_id in self.trick_cards:\n",
    "            if c_id in self.mendi_suit_ids: self.mendis_captured[winner_team] += 1\n",
    "        self.current_player_index = winner_idx\n",
    "\n",
    "    def is_game_over(self): return sum(self.tricks_captured.values()) >= self.cards_per_player\n",
    "\n",
    "    def set_trump(self, trump_card_id, declarer_player_id):\n",
    "        self.trump_suit = get_rank_suit_from_id(trump_card_id)[1]\n",
    "        self.trump_declarer_team = self.get_team(declarer_player_id)\n",
    "\n",
    "    def get_final_rewards(self):\n",
    "        m0, m1, t0, t1, r = self.mendis_captured[0], self.mendis_captured[1], self.tricks_captured[0], self.tricks_captured[1], 0\n",
    "        if t0 == self.cards_per_player: r = 1000\n",
    "        elif t1 == self.cards_per_player: r = -1000\n",
    "        elif m0 == 4: r = 600\n",
    "        elif m1 == 4: r = -600\n",
    "        elif m0 == 3: r = 300\n",
    "        elif m1 == 3: r = -300\n",
    "        elif m0 == 2:\n",
    "            if t0 > t1: r = 150\n",
    "            elif t1 > t0: r = -150\n",
    "            else:\n",
    "                if self.trump_declarer_team == 0: r = -150\n",
    "                elif self.trump_declarer_team == 1: r = 150\n",
    "        return {0: r, 1: -r}\n",
    "\n",
    "    def clone(self):\n",
    "        new = GameState(self.num_players, self.dealer_index)\n",
    "        new.current_player_index, new.hands = self.current_player_index, {p: h[:] for p, h in self.hands.items()}\n",
    "        new.trick_cards, new.trick_suits_led, new.trump_suit = self.trick_cards[:], self.trick_suits_led.copy(), self.trump_suit\n",
    "        new.trump_declarer_team, new.mendis_captured, new.tricks_captured = self.trump_declarer_team, self.mendis_captured.copy(), self.tricks_captured.copy()\n",
    "        new.current_trick_lead_player, new.current_trick_lead_suit = self.current_trick_lead_player, self.current_trick_lead_suit\n",
    "        # CORRECTED CLONE METHOD\n",
    "        new.card_owner_history = self.card_owner_history.copy()\n",
    "        new.void_suits = {p: s.copy() for p, s in self.void_suits.items()}\n",
    "        return new\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, parent=None, prior_p=1.0):\n",
    "        self.parent, self.children, self.n_visits, self.q_value, self.prior_p = parent, {}, 0, 0.0, prior_p\n",
    "    def expand(self, action_priors):\n",
    "        for action, prob in action_priors.items():\n",
    "            if action not in self.children: self.children[action] = MCTSNode(parent=self, prior_p=prob)\n",
    "    def select(self, c_puct): return max(self.children.items(), key=lambda act_node: act_node[1].get_ucb_score(c_puct))\n",
    "    def update(self, leaf_value):\n",
    "        self.n_visits += 1; self.q_value += (leaf_value - self.q_value) / self.n_visits\n",
    "    def get_ucb_score(self, c_puct):\n",
    "        u_value = c_puct * self.prior_p * math.sqrt(self.parent.n_visits) / (1 + self.n_visits)\n",
    "        return self.q_value + u_value\n",
    "    def is_leaf(self): return len(self.children) == 0\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model, c_puct=1.0, n_simulations=100):\n",
    "        self.model, self.c_puct, self.n_simulations = model, c_puct, n_simulations\n",
    "        self.state_to_tensor_func = None\n",
    "    def _playout(self, state, node):\n",
    "        while not node.is_leaf():\n",
    "            action, node = node.select(self.c_puct); state.play_card(state.current_player_index, action)\n",
    "        if not state.is_game_over():\n",
    "            player_id = state.current_player_index\n",
    "            player_hand, legal_moves = state.hands[player_id], state.get_legal_moves()\n",
    "            if not legal_moves: leaf_value = state.get_final_rewards()[state.get_team(player_id)]\n",
    "            else:\n",
    "                state_tensor = self.state_to_tensor_func(state, player_id)\n",
    "                with torch.no_grad(): policy_logits, leaf_value_tensor = self.model(state_tensor)\n",
    "                leaf_value = leaf_value_tensor.item()\n",
    "                hand_map = {card_id: i for i, card_id in enumerate(player_hand)}\n",
    "                legal_hand_indices = [hand_map[card_id] for card_id in legal_moves]\n",
    "                legal_logits = policy_logits[0, legal_hand_indices]\n",
    "                action_probs = F.softmax(legal_logits, dim=0).cpu().numpy()\n",
    "                node.expand({move: prob for move, prob in zip(legal_moves, action_probs)})\n",
    "        else: leaf_value = state.get_final_rewards()[state.get_team(state.current_player_index)]\n",
    "        curr_node = node\n",
    "        while curr_node is not None: curr_node.update(leaf_value); curr_node = curr_node.parent\n",
    "    def get_move_probs(self, state, state_to_tensor_func, temp=1e-3):\n",
    "        self.state_to_tensor_func = state_to_tensor_func\n",
    "        root = MCTSNode()\n",
    "        player_id = state.current_player_index\n",
    "        player_hand, legal_moves = state.hands[player_id], state.get_legal_moves()\n",
    "        if not legal_moves: return [], torch.tensor([])\n",
    "        if len(legal_moves) == 1: return legal_moves, torch.tensor([1.0])\n",
    "        \n",
    "        state_tensor = self.state_to_tensor_func(state, player_id)\n",
    "        with torch.no_grad(): policy_logits, _ = self.model(state_tensor)\n",
    "        \n",
    "        hand_map = {card_id: i for i, card_id in enumerate(player_hand)}\n",
    "        legal_hand_indices = [hand_map[card_id] for card_id in legal_moves]\n",
    "        legal_logits = policy_logits[0, legal_hand_indices]\n",
    "        action_probs = F.softmax(legal_logits, dim=0).cpu().numpy()\n",
    "        \n",
    "        alpha, epsilon = 0.3, 0.25\n",
    "        noise = np.random.dirichlet([alpha] * len(action_probs))\n",
    "        noisy_probs = (1 - epsilon) * action_probs + epsilon * noise\n",
    "        \n",
    "        root.expand({move: prob for move, prob in zip(legal_moves, noisy_probs)})\n",
    "        \n",
    "        for _ in range(self.n_simulations): self._playout(state.clone(), root)\n",
    "        \n",
    "        act_visits = [(act, node.n_visits) for act, node in root.children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = F.softmax(torch.tensor(visits, dtype=torch.float32) / temp, dim=0)\n",
    "        return acts, act_probs\n",
    "\n",
    "class MendikotModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size, num_players=4):\n",
    "        super(MendikotModel, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(nn.Linear(state_size, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU())\n",
    "        self.policy_head, self.value_head = nn.Linear(512, action_size), nn.Linear(512, 1)\n",
    "    def forward(self, state_tensor):\n",
    "        x = self.fc_layers(state_tensor); return self.policy_head(x), torch.tanh(self.value_head(x))\n",
    "\n",
    "class MendikotTrainer:\n",
    "    def __init__(self, num_players, state_size, action_size):\n",
    "        self.num_players, self.state_size, self.action_size = num_players, state_size, action_size\n",
    "        self.model = MendikotModel(state_size, action_size, num_players).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "        run_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        self.run_models_dir = os.path.join('models', run_name)\n",
    "        os.makedirs(self.run_models_dir, exist_ok=True)\n",
    "        print(f\"--- Starting new training run: {run_name} ---\\nModels will be saved in: {self.run_models_dir}\")\n",
    "    def state_to_tensor(self, game_state, player_id):\n",
    "        my_hand_vec = np.zeros(48); my_hand_vec[game_state.hands[player_id]] = 1\n",
    "        card_owners_vec = np.zeros((48, 4))\n",
    "        for card_idx in range(48):\n",
    "            owner = game_state.card_owner_history[card_idx]\n",
    "            if owner > 0: card_owners_vec[card_idx, owner - 1] = 1\n",
    "        card_owners_vec = card_owners_vec.flatten()\n",
    "        void_vec = np.zeros((4, 4))\n",
    "        suit_map = {'H': 0, 'D': 1, 'C': 2, 'S': 3}\n",
    "        for p_id in range(4):\n",
    "            for suit_char in game_state.void_suits[p_id]: void_vec[p_id, suit_map[suit_char]] = 1\n",
    "        void_vec = void_vec.flatten()\n",
    "        trump_vec, lead_suit_vec = np.zeros(4), np.zeros(4)\n",
    "        if game_state.trump_suit is not None: trump_vec[suit_map.get(game_state.trump_suit, 0)] = 1\n",
    "        if game_state.current_trick_lead_suit is not None: lead_suit_vec[suit_map.get(game_state.current_trick_lead_suit, 0)] = 1\n",
    "        player_info = np.array([player_id/4.0, game_state.current_player_index/4.0, game_state.dealer_index/4.0, len(game_state.trick_cards)/4.0])\n",
    "        scores_vec = np.array([game_state.mendis_captured[0]/4.0, game_state.tricks_captured[0]/self.action_size, game_state.mendis_captured[1]/4.0, game_state.tricks_captured[1]/self.action_size])\n",
    "        state_vector = np.concatenate([my_hand_vec, card_owners_vec, void_vec, trump_vec, lead_suit_vec, player_info, scores_vec])\n",
    "        return torch.FloatTensor(state_vector).unsqueeze(0).to(device)\n",
    "    def choose_action(self, game_state, temp=1.0):\n",
    "        mcts = MCTS(self.model, n_simulations=100)\n",
    "        legal_moves, move_probs = mcts.get_move_probs(game_state, self.state_to_tensor, temp=temp)\n",
    "        if not legal_moves: return None, None\n",
    "        chosen_idx = np.random.choice(len(legal_moves), p=move_probs.numpy())\n",
    "        chosen_card = legal_moves[chosen_idx]\n",
    "        move_probs_for_learning = torch.zeros(self.action_size, device=device)\n",
    "        hand_map = {card_id: i for i, card_id in enumerate(game_state.hands[game_state.current_player_index])}\n",
    "        for move, prob in zip(legal_moves, move_probs):\n",
    "            if move in hand_map: move_probs_for_learning[hand_map[move]] = prob\n",
    "        return chosen_card, move_probs_for_learning\n",
    "    def run_episode(self):\n",
    "        game = GameState(num_players=self.num_players)\n",
    "        episode_history = []\n",
    "        while not game.is_game_over():\n",
    "            player_id = game.current_player_index\n",
    "            action_card, move_probs = self.choose_action(game)\n",
    "            if action_card is None: break\n",
    "            state_tensor = self.state_to_tensor(game, player_id)\n",
    "            episode_history.append({'state': state_tensor, 'policy': move_probs, 'player': player_id})\n",
    "            game.play_card(player_id, action_card)\n",
    "        final_rewards = game.get_final_rewards()\n",
    "        for step in episode_history:\n",
    "            team_id = game.get_team(step['player'])\n",
    "            step['reward'] = final_rewards[team_id]\n",
    "            self.replay_buffer.push(step['state'], step['policy'], step['reward'])\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE: return\n",
    "        batch = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        states, target_policies, rewards = zip(*batch)\n",
    "        states_tensor, target_policies_tensor, rewards_tensor = torch.cat(states).to(device), torch.stack(target_policies).to(device), torch.FloatTensor(rewards).to(device)\n",
    "        pred_policies_logits, pred_values = self.model(states_tensor)\n",
    "        pred_values = pred_values.squeeze()\n",
    "        value_loss = F.mse_loss(pred_values, rewards_tensor)\n",
    "        policy_loss = F.cross_entropy(pred_policies_logits, target_policies_tensor)\n",
    "        total_loss = policy_loss + value_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "    def train(self):\n",
    "        for episode in tqdm(range(NUM_EPISODES)):\n",
    "            self.run_episode()\n",
    "            self.learn()\n",
    "            if (episode + 1) % SAVE_EVERY == 0:\n",
    "                save_path = os.path.join(self.run_models_dir, f\"mendikot_model_ep_{episode+1}.pth\")\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "# ------------------- Part 5: MAIN EXECUTION -------------------\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Hyperparameters Set for Training ---\")\n",
    "    print(f\"State Size: {STATE_SIZE}, Action Size: {ACTION_SIZE}\")\n",
    "    print(f\"Total Episodes: {NUM_EPISODES}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    trainer = MendikotTrainer(num_players=4, state_size=STATE_SIZE, action_size=ACTION_SIZE)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d981d6-2654-4908-8a12-6d179bd2e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#               IMPROVED LOCAL SETUP CELL\n",
    "#  This cell configures the environment for local execution.\n",
    "# ==========================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Change the working directory to the project root\n",
    "#    This is the key to finding the 'src' and 'models' folders.\n",
    "#    '..' means \"the parent directory\".\n",
    "#    This handles the case where the notebook is run from the 'notebooks' subfolder.\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print(f\"Current Working Directory set to: {os.getcwd()}\")\n",
    "\n",
    "# 2. Add the project root to the Python path for imports\n",
    "#    This allows 'from src.game import GameState' to work.\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.')\n",
    "\n",
    "# 3. Define the device\n",
    "print(\"Setting device to CPU for local execution.\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 4. Create the 'models' directory if it doesn't exist (as a safety check)\n",
    "models_dir = 'D:/mendikot_ai_project/models'\n",
    "if not os.path.exists(models_dir):\n",
    "    print(f\"Warning: Directory '{models_dir}' not found. Creating it...\")\n",
    "    os.makedirs(models_dir)\n",
    "else:\n",
    "    print(f\"Directory '{models_dir}' found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d77d16-bc60-4917-97e3-7c04fac04509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "#          FINAL DEBUG & ANALYSIS CELL\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- DIAGNOSTICS: Let's find out where we are ---\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "try:\n",
    "    print(f\"Contents of 'models' directory: {os.listdir('models')}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find the 'models' directory from the current location.\")\n",
    "print(\"-\" * 20)\n",
    "# --- END OF DIAGNOSTICS ---\n",
    "\n",
    "# Ensure our custom classes from 'src' are available\n",
    "# If you are running from the 'notebooks' folder, this is necessary.\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..') \n",
    "from src.game import GameState\n",
    "from src.agent import MendikotModel\n",
    "\n",
    "# --- Configuration ---\n",
    "# ---!! IMPORTANT !! ---\n",
    "# ---!! COPY THE EXACT FOLDER NAME FROM THE OUTPUT ABOVE !! ---\n",
    "RUN_FOLDER_NAME = '2025-10-24_20-15-23'  # <-- Double-check this name!\n",
    "CHECKPOINT_FILE_NAME = 'mendikot_model_ep_3000.pth' # <-- Check if this file exists!\n",
    "\n",
    "# Build the path in an OS-safe way\n",
    "MODEL_CHECKPOINT_TO_TEST = os.path.join('models', RUN_FOLDER_NAME, CHECKPOINT_FILE_NAME)\n",
    "\n",
    "NUM_GAMES_TO_SIMULATE = 500\n",
    "AI_PLAYER_ID = 0\n",
    "STATE_SIZE = 112\n",
    "ACTION_SIZE = 12\n",
    "device = torch.device('cpu')\n",
    "print(f\"Analysis running on device: {device}\")\n",
    "\n",
    "# (The rest of the code is the same as before...)\n",
    "# ... (state_to_tensor, RandomAgent, load_trained_agent, evaluate_model, and the final execution block)\n",
    "def state_to_tensor(game_state, player_id):\n",
    "    my_hand_vec = np.zeros(48); my_hand_vec[game_state.hands[player_id]] = 1\n",
    "    played_history_vec = game_state.played_cards_history\n",
    "    suit_map = {'H': 0, 'D': 1, 'C': 2, 'S': 3}\n",
    "    trump_vec, lead_suit_vec = np.zeros(4), np.zeros(4)\n",
    "    if game_state.trump_suit is not None: trump_vec[suit_map.get(game_state.trump_suit, 0)] = 1\n",
    "    if game_state.current_trick_lead_suit is not None: lead_suit_vec[suit_map.get(game_state.current_trick_lead_suit, 0)] = 1\n",
    "    player_info = np.array([player_id/4.0, game_state.current_player_index/4.0, game_state.dealer_index/4.0, len(game_state.trick_cards)/4.0])\n",
    "    scores_vec = np.array([game_state.mendis_captured[0]/4.0, game_state.tricks_captured[0]/ACTION_SIZE, game_state.mendis_captured[1]/4.0, game_state.tricks_captured[1]/ACTION_SIZE])\n",
    "    state_vector = np.concatenate([my_hand_vec, played_history_vec, trump_vec, lead_suit_vec, player_info, scores_vec])\n",
    "    return torch.FloatTensor(state_vector).unsqueeze(0)\n",
    "class RandomAgent:\n",
    "    def choose_action(self, game_state):\n",
    "        legal_moves = game_state.get_legal_moves()\n",
    "        return np.random.choice(legal_moves) if legal_moves else None\n",
    "def load_trained_agent(model_path, state_size, action_size, num_players):\n",
    "    print(f\"Attempting to load model from: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"--> ERROR: Model file not found at the specified path.\"); return None\n",
    "    model = MendikotModel(state_size, action_size, num_players)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "def evaluate_model(model, num_games, ai_player_id):\n",
    "    wins, total_rewards = 0, []\n",
    "    random_agent = RandomAgent()\n",
    "    for _ in tqdm(range(num_games), desc=\"Evaluating Model\"):\n",
    "        game = GameState(num_players=4)\n",
    "        ai_team = game.get_team(ai_player_id)\n",
    "        while not game.is_game_over():\n",
    "            player_id = game.current_player_index\n",
    "            chosen_card = None\n",
    "            if game.get_team(player_id) == ai_team:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = state_to_tensor(game, player_id).to(device)\n",
    "                    player_hand = game.hands[player_id]\n",
    "                    legal_moves = game.get_legal_moves()\n",
    "                    if not legal_moves: break\n",
    "                    policy_logits, _ = model(state_tensor)\n",
    "                    hand_map = {card_id: i for i, card_id in enumerate(player_hand)}\n",
    "                    legal_hand_indices = [hand_map[card_id] for card_id in legal_moves if card_id in hand_map]\n",
    "                    best_action_idx = -1; best_logit = -float('inf')\n",
    "                    for idx in legal_hand_indices:\n",
    "                        if policy_logits[0, idx] > best_logit:\n",
    "                            best_logit = policy_logits[0, idx]; best_action_idx = idx\n",
    "                    if best_action_idx != -1: chosen_card = player_hand[best_action_idx]\n",
    "            else:\n",
    "                chosen_card = random_agent.choose_action(game)\n",
    "            if chosen_card is None: break\n",
    "            game.play_card(player_id, chosen_card)\n",
    "        ai_reward = game.get_final_rewards()[ai_team]\n",
    "        total_rewards.append(ai_reward)\n",
    "        if ai_reward > 0: wins += 1\n",
    "    return wins, total_rewards\n",
    "if __name__ == '__main__':\n",
    "    trained_model = load_trained_agent(MODEL_CHECKPOINT_TO_TEST, STATE_SIZE, ACTION_SIZE, 4)\n",
    "    if trained_model:\n",
    "        wins, rewards = evaluate_model(trained_model, NUM_GAMES_TO_SIMULATE, AI_PLAYER_ID)\n",
    "        win_rate = (wins / NUM_GAMES_TO_SIMULATE) * 100\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Model: {MODEL_CHECKPOINT_TO_TEST}\")\n",
    "        print(f\"Games Played: {NUM_GAMES_TO_SIMULATE}\")\n",
    "        print(f\"Win Rate vs. Random Agents: {win_rate:.2f}%\")\n",
    "        print(f\"Average Reward per Game: {avg_reward:.2f}\")\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        sns.histplot(rewards, bins=20, kde=True, ax=ax1, color='skyblue', label='Reward Distribution')\n",
    "        ax1.set_title(f'Reward Distribution for Model: {os.path.basename(MODEL_CHECKPOINT_TO_TEST)}', fontsize=16)\n",
    "        ax1.set_xlabel('Final Reward', fontsize=12); ax1.set_ylabel('Number of Games', fontsize=12)\n",
    "        ax1.axvline(avg_reward, color='red', linestyle='--', label=f'Avg Reward: {avg_reward:.2f}')\n",
    "        ax1.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax1.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56436d5-bf24-4f18-aa67-7c11289e3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "#          FINAL MODEL ANALYSIS AND VISUALIZATION CELL\n",
    "#       (Completely independent of the training script)\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Imports for this cell\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Ensure our custom classes from the 'src' folder are available\n",
    "sys.path.append('..') \n",
    "from src.game import GameState\n",
    "from src.agent import MendikotModel\n",
    "\n",
    "# --- Configuration for the Analysis ---\n",
    "MODEL_CHECKPOINT_TO_TEST = 'D:/mendikot_ai_project/models/2025-10-24_20-15-23/mendikot_model_ep_3000.pth'\n",
    "NUM_GAMES_TO_SIMULATE = 500\n",
    "AI_PLAYER_ID = 0\n",
    "STATE_SIZE = 112\n",
    "ACTION_SIZE = 12\n",
    "\n",
    "# --- NEW: DEFINE THE DEVICE ---\n",
    "# We are running this locally, so we'll use the CPU.\n",
    "device = torch.device('cpu')\n",
    "print(f\"Analysis running on device: {device}\")\n",
    "# --- END OF NEW ---\n",
    "\n",
    "# 2. Define a standalone state_to_tensor helper function\n",
    "def state_to_tensor(game_state, player_id):\n",
    "    # ... (the function is the same as before, no changes needed)\n",
    "    my_hand_vec = np.zeros(48); my_hand_vec[game_state.hands[player_id]] = 1\n",
    "    played_history_vec = game_state.played_cards_history\n",
    "    suit_map = {'H': 0, 'D': 1, 'C': 2, 'S': 3}\n",
    "    trump_vec, lead_suit_vec = np.zeros(4), np.zeros(4)\n",
    "    if game_state.trump_suit is not None: trump_vec[suit_map.get(game_state.trump_suit, 0)] = 1\n",
    "    if game_state.current_trick_lead_suit is not None: lead_suit_vec[suit_map.get(game_state.current_trick_lead_suit, 0)] = 1\n",
    "    player_info = np.array([player_id/4.0, game_state.current_player_index/4.0, game_state.dealer_index/4.0, len(game_state.trick_cards)/4.0])\n",
    "    scores_vec = np.array([game_state.mendis_captured[0]/4.0, game_state.tricks_captured[0]/ACTION_SIZE, game_state.mendis_captured[1]/4.0, game_state.tricks_captured[1]/ACTION_SIZE])\n",
    "    state_vector = np.concatenate([my_hand_vec, played_history_vec, trump_vec, lead_suit_vec, player_info, scores_vec])\n",
    "    return torch.FloatTensor(state_vector).unsqueeze(0)\n",
    "\n",
    "# 3. Define a simple Baseline Agent\n",
    "class RandomAgent:\n",
    "    def choose_action(self, game_state):\n",
    "        legal_moves = game_state.get_legal_moves()\n",
    "        return np.random.choice(legal_moves) if legal_moves else None\n",
    "\n",
    "# 4. Function to load our trained AI model\n",
    "def load_trained_agent(model_path, state_size, action_size, num_players):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERROR: Model file not found at {model_path}\"); return None\n",
    "    model = MendikotModel(state_size, action_size, num_players)\n",
    "    # The 'device' variable will now be correctly found\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device) # Move the model to the specified device\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 5. The main evaluation loop\n",
    "def evaluate_model(model, num_games, ai_player_id):\n",
    "    wins, total_rewards = 0, []\n",
    "    random_agent = RandomAgent()\n",
    "    for _ in tqdm(range(num_games), desc=\"Evaluating Model\"):\n",
    "        game = GameState(num_players=4)\n",
    "        ai_team = game.get_team(ai_player_id)\n",
    "        while not game.is_game_over():\n",
    "            player_id = game.current_player_index\n",
    "            chosen_card = None\n",
    "            if game.get_team(player_id) == ai_team:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = state_to_tensor(game, player_id).to(device) # Ensure tensor is on the correct device\n",
    "                    player_hand = game.hands[player_id]\n",
    "                    legal_moves = game.get_legal_moves()\n",
    "                    if not legal_moves: break\n",
    "                    \n",
    "                    policy_logits, _ = model(state_tensor)\n",
    "                    hand_map = {card_id: i for i, card_id in enumerate(player_hand)}\n",
    "                    legal_hand_indices = [hand_map[card_id] for card_id in legal_moves if card_id in hand_map]\n",
    "                    \n",
    "                    best_action_idx = -1; best_logit = -float('inf')\n",
    "                    for idx in legal_hand_indices:\n",
    "                        if policy_logits[0, idx] > best_logit:\n",
    "                            best_logit = policy_logits[0, idx]; best_action_idx = idx\n",
    "                    \n",
    "                    if best_action_idx != -1: chosen_card = player_hand[best_action_idx]\n",
    "            else:\n",
    "                chosen_card = random_agent.choose_action(game)\n",
    "            if chosen_card is None: break\n",
    "            game.play_card(player_id, chosen_card)\n",
    "            \n",
    "        ai_reward = game.get_final_rewards()[ai_team]\n",
    "        total_rewards.append(ai_reward)\n",
    "        if ai_reward > 0: wins += 1\n",
    "\n",
    "        gc.collect()\n",
    "    return wins, total_rewards\n",
    "\n",
    "# 6. Run the evaluation and print/plot results\n",
    "if __name__ == '__main__':\n",
    "    trained_model = load_trained_agent(MODEL_CHECKPOINT_TO_TEST, STATE_SIZE, ACTION_SIZE, 4)\n",
    "    if trained_model:\n",
    "        wins, rewards = evaluate_model(trained_model, NUM_GAMES_TO_SIMULATE, AI_PLAYER_ID)\n",
    "        win_rate = (wins / NUM_GAMES_TO_SIMULATE) * 100\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Model: {MODEL_CHECKPOINT_TO_TEST}\")\n",
    "        print(f\"Games Played: {NUM_GAMES_TO_SIMULATE}\")\n",
    "        print(f\"Win Rate vs. Random Agents: {win_rate:.2f}%\")\n",
    "        print(f\"Average Reward per Game: {avg_reward:.2f}\")\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        sns.histplot(rewards, bins=20, kde=True, ax=ax1, color='skyblue', label='Reward Distribution')\n",
    "        ax1.set_title(f'Reward Distribution for Model: {os.path.basename(MODEL_CHECKPOINT_TO_TEST)}', fontsize=16)\n",
    "        ax1.set_xlabel('Final Reward', fontsize=12); ax1.set_ylabel('Number of Games', fontsize=12)\n",
    "        ax1.axvline(avg_reward, color='red', linestyle='--', label=f'Avg Reward: {avg_reward:.2f}')\n",
    "        ax1.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax1.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6c914-5129-4f81-9971-e9c763625bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
